import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd

# Generating random linear data
x = np.linspace(0, 50, 50)
y = np.linspace(0, 50, 50)

# Add some noise to the data
x += np.random.uniform(-4, 4, 50)
y += np.random.uniform(-4, 4, 50)
n = len(x)

# Plot of Training Data
plt.scatter(x, y)
plt.xlabel('x')
plt.ylabel('y')
plt.title("Training Data")
plt.show()

# Create a model for the data
X = tf.constant(x, dtype=tf.float32)
Y = tf.constant(y, dtype=tf.float32)

# Initialize variables (weights and bias)
W = tf.Variable(np.random.randn(), name="W", dtype=tf.float32)
b = tf.Variable(np.random.randn(), name="b", dtype=tf.float32)

# Set learning rate
learning_rate = 0.01

# Number of training epochs
training_epochs = 100

# Linear regression model
def linear_regression(x):
    return W * x + b

# Mean Squared Error Cost Function
def mean_squared_error(y_true, y_pred):
    return tf.reduce_mean(tf.square(y_true - y_pred) / 2)

# Optimization using Gradient Descent
optimizer = tf.optimizers.SGD(learning_rate)

# Training loop
for epoch in range(training_epochs):
    with tf.GradientTape() as tape:
        predictions = linear_regression(X)
        loss = mean_squared_error(Y, predictions)
    
    gradients = tape.gradient(loss, [W, b])
    optimizer.apply_gradients(zip(gradients, [W, b]))
    
    if (epoch + 1) % 50 == 0:
        print("Epoch", (epoch + 1), ": loss =", loss.numpy(), "W =", W.numpy(), "b =", b.numpy())

# Training cost
training_cost = mean_squared_error(Y, linear_regression(X)).numpy()
weight = W.numpy()
bias = b.numpy()

# Calculate predictions
predictions = weight * x + bias
print("Training cost =", training_cost, "Weight =", weight, "bias =", bias, '\n')

# Plotting the Results
plt.plot(x, y, 'ro', label='Original data')
plt.plot(x, predictions, label='Fitted line')
plt.title('Linear Regression Result')
plt.legend()
plt.show()
