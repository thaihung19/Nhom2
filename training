import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd

# Load data from CSV file
df = pd.read_csv('Student_Performance.csv')

# Split the data into training and testing sets (50% training, 50% testing)
split_ratio = 0.5
split_index = int(len(df) * split_ratio)
train_data = df.iloc[:split_index]
test_data = df.iloc[split_index:]

# Extract features and target from the data
train_x = train_data.drop('Performance Index', axis=1).values  # Chuyển dữ liệu thành mảng NumPy
train_y = train_data['Performance Index'].values

# Create a model for the data
X = tf.constant(train_x, dtype=tf.float32)
Y = tf.constant(train_y, dtype=tf.float32)

# Initialize variables (weights and bias)
W = tf.Variable(np.random.randn(train_x.shape[1], 1), name="W", dtype=tf.float32)  # Sử dụng train_x.shape[1] thay vì len(train_x.columns)
b = tf.Variable(np.random.randn(), name="b", dtype=tf.float32)

# Set learning rate
learning_rate = 0.01

# Number of training epochs
training_epochs = 100


# Linear regression model
def linear_regression(x):
    return tf.matmul(x, W) + b


# Mean Squared Error Cost Function
def mean_squared_error(y_true, y_pred):
    return tf.reduce_mean(tf.square(y_true - y_pred) / 2)


# Optimization using Gradient Descent
optimizer = tf.optimizers.SGD(learning_rate)

# Training loop
for epoch in range(training_epochs):
    with tf.GradientTape() as tape:
        predictions = linear_regression(X)
        loss = mean_squared_error(Y, predictions)

    gradients = tape.gradient(loss, [W, b])
    optimizer.apply_gradients(zip(gradients, [W, b]))

    if (epoch + 1) % 50 == 0:
        print("Epoch", (epoch + 1), ": loss =", loss.numpy())

# Use the trained model to make predictions on the test set
test_x = tf.constant(test_data.drop('Performance Index', axis=1).values, dtype=tf.float32)

# Calculate the test loss
test_predictions = linear_regression(test_x)
test_loss = mean_squared_error(test_data['Performance Index'].values, test_predictions)

print("Test loss =", test_loss.numpy())

